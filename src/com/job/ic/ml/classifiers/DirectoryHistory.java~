package com.job.ic.ml.classifiers;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import org.apache.log4j.Logger;

import com.job.ic.crawlers.HttpSegmentCrawler;
import com.job.ic.crawlers.models.CrawlerConfig;
import com.job.ic.crawlers.models.DirectoryTree;
import com.job.ic.crawlers.models.DirectoryTreeNode;
import com.job.ic.crawlers.models.SegmentQueueModel;
import com.job.ic.experiments.ParseLog;
import com.job.ic.utils.FileUtils;
import com.job.ic.utils.HttpUtils;
import com.job.ic.utils.StringUtils;
import com.sleepycat.je.cleaner.OffsetList.Segment;

import weka.core.Instances;

public class DirectoryHistory {
	private static Logger logger = Logger.getLogger(DirectoryHistory.class);

	private static ConcurrentHashMap<String, DirectoryTree> hostData = new ConcurrentHashMap<String, DirectoryTree>();

	private static ArrayList<String> rel = new ArrayList<>();
	private static ArrayList<String> non = new ArrayList<>();
	private static ArrayList<String> all = new ArrayList<>();

	private static ArrayList<String> allDup = new ArrayList<>();
	private static ArrayList<String> results = new ArrayList<>();

	// private static ArrayList<String> previousHistory = new ArrayList<>();

	private static final boolean useHistoryPredictor = CrawlerConfig.getConfig().useHistoryPredictor();

	private static HashMap<String, HashSet<String>> dupDb = new HashMap<>();
	// private static HashMap<String, String> dupDb = new HashMap<>();

	private static final String header = "@relation 'history'" + "\n@attribute segpath string\n" + "@attribute HostrelPagesRatio numeric\n" + "@attribute HostrelSegsRatio numeric\n"
			+ "@attribute relPagesRatio numeric\n" + "@attribute relSegsRatio numeric\n" + "@attribute ParentRelPagesRatio numeric\n" + "@attribute ParentRelSegsRatio numeric\n"
			+ "@attribute ChildRelPagesRatio numeric\n" + "@attribute ChildRelSegsRatio numeric\n" + "@attribute SiblingRelPagesRatio numeric\n" + "@attribute SiblingRelSegsRatio numeric\n"
			+ "@attribute class {thai,non}\n" + "@data";

	private static WekaClassifier historyPredictor = null;

	public static boolean useHistoryPredictor() {
		return useHistoryPredictor;
	}

	public static String getHistoryFeaturesHeader() {
		return header;
	}

	public static DirectoryTree get(String hostname) {
		if (hostname == null)
			return null;

		return hostData.get(hostname);
	}

	public static void clear() {
		hostData.clear();
	}

	public static synchronized void record(SegmentQueueModel input, int relPages, int nonPages) {
		record(input.getSegmentName(), relPages, nonPages);
	}

	public static synchronized void record(String segmentName, int relPages, int nonPages) {

		boolean isRelevant = false;
		double degree = HttpSegmentCrawler.calcRelevanceDegree(relPages, nonPages);
		if (degree > CrawlerConfig.getConfig().getRelevanceDegreeThreshold()) {
			isRelevant = true;
		}

		DirectoryTreeNode node = getDirectoryNode(segmentName);

		ClassifierOutput predict = node.getDirPrediction();
		
		if (predict != null) {
			if (predict.getResultClass() == ResultClass.RELEVANT) {
				if (isRelevant) {
					AccuracyTracker.getHistoryConfusionMatrix().incTp();
				} else {
					AccuracyTracker.getHistoryConfusionMatrix().incFn();
				}
			} else {
				if (isRelevant) {
					AccuracyTracker.getHistoryConfusionMatrix().incFp();
				} else {
					AccuracyTracker.getHistoryConfusionMatrix().incTn();
				}
			}

			if (relPages + nonPages > 0) {
				String features = predict.getFeatures();// getDirectoryFeatures(node,
														// isRelevant);
				if (isRelevant) {
					if (features.lastIndexOf(",non") < 0)
						return;
					features = features.substring(0, features.lastIndexOf(",non")) + ",thai";
				}

				String host = HttpUtils.getHost(segmentName);

				allDup.add(features);
				System.err.println(features);
				if (host != null) {
					if (!dupDb.containsKey(host)) {
						dupDb.put(host, new HashSet<>());
					}

					String tmp = features.substring(features.indexOf(",") + 1);

					if(StringUtils.countWordInStr(tmp, ",") == 11){
						if(StringUtils.countWordInStr(tmp, "?") == 10)
							return;
					}else{
						if(StringUtils.countWordInStr(tmp, "?") == 16)
							return;
					}
//					if (tmp.contains("?,?,?,?,?,?,?,?,?,?,")) {
//						return;
//					}

					// TODO: Dup
					// if (dupDb.get(host).contains(tmp)) {
					// return;
					// }
					//
					// dupDb.get(host).add(tmp);
				}

				if (features == null)
					return;


				String previous = segmentName + "\t" + predict.getExtra() + "\t" + isRelevant;
				// previousHistory.add(previous);

				if (isRelevant) {
					rel.add(features);
				} else {
					non.add(features);
				}
				all.add(features);

				results.add(features + "\tPrediction:\t" + predict.getResultClass() + "\t" + predict.getRelevantScore() + "\tActual:\t" + isRelevant);
			}
		}

		if (relPages + nonPages > 0)
			node.getDirectoryTree().addCrawledNode(segmentName, relPages, nonPages);

	}

	public static synchronized void onlineUpdate() {
		backupAllHistoryData("logs/history.arff");

		Instances hi;
		ArrayList<String> historyDataset = createHistoryDataSet();
		if (historyDataset != null) {
			logger.info("History Size : " + historyDataset.size());
			FileUtils.deleteFile("logs/htmp.arff");
			FileUtils.writeTextFile("logs/htmp.arff", DirectoryHistory.getHistoryFeaturesHeader() + "\n", false);
			FileUtils.writeTextFile("logs/htmp.arff", historyDataset, true);

			FileUtils.writeTextFile("logs/hupdate.arff", historyDataset, true);
			FileUtils.writeTextFile("logs/hupdate.arff", "--------------------------------------------------------------------------------\n", true);

			
			if(CrawlerConfig.getConfig().isTrainingMode())
				return;
			
			try {
				hi = new Instances(new FileReader("logs/htmp.arff"));
				hi.setClassIndex(hi.numAttributes() - 1);

				if (historyPredictor == null)
					return;

				// TODO: Set weight;
				// historyPredictor.setWeight(AccuracyTracker.calcWeight(AccuracyTracker.getHistoryConfusionMatrix().getAccuracy()));

				
				historyPredictor.updateClassifier(hi);
			} catch (Exception e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

		}

	}

	public static synchronized ClassifierOutput predict(String basePath) {
		if (basePath == null)
			return null;

		DirectoryTreeNode node = getDirectoryNode(basePath);
		if (node == null) {
			// logger.info("cannot find node");
			return null;
		}

		// String hostPages = "[" +
		// node.getDirectoryTree().getHostHistoryPages().stream().map(s ->
		// String.valueOf(s)).collect(Collectors.joining(",")) + "]";
		// String hostSegs = "[" +
		// node.getDirectoryTree().getHostHistorySegs().stream().map(s ->
		// String.valueOf(s)).collect(Collectors.joining(",")) + "]";

		// String pathPages = "[" + node.getHistoryPages().stream().map(s ->
		// String.valueOf(s)).collect(Collectors.joining(",")) + "]";
		// String pathSegs = "[" + node.getHistoryPages().stream().map(s ->
		// String.valueOf(s)).collect(Collectors.joining(",")) + "]";

		// String extra = hostPages + hostSegs + pathPages + pathSegs;

		String features = getDirectoryFeatures(node);
		if (features == null) {
			return null;
		}


		if (CrawlerConfig.getConfig().isTrainingMode()) {
			ClassifierOutput output = new ClassifierOutput(0.5, 0.5, 1.0);
			output.setFeatures(features);
			node.setDirPrediction(output);
			return output;
		}

		if (useHistoryPredictor == false)
			return null;

		String[] tmp = features.split(",");
		ClassifierOutput output = historyPredictor.predict(basePath, tmp);
		output.setFeatures(features);
		// output.setExtra(extra);

		double total = tmp.length - 1;
		// output.setWeight(historyPredictor.getWeight() * (total -
		// StringUtils.countWordInStr(features, "?")) / total);
		output.setWeight(1);
		// System.out.println(output.getWeight() + "\t" +
		// output.getRelevantScore());

		if (total == 0)
			return null;

		node.setDirPrediction(output);
		return output;
	}

	private static synchronized DirectoryTreeNode getDirectoryNode(String basePath) {
		// if (!useHistoryPredictor)
		// return null;

		DirectoryTree tree = null;
		DirectoryTreeNode node = null;
		String hostname = HttpUtils.getHost(basePath);

		if (hostname == null)
			return null;

		if (hostData.containsKey(hostname.toLowerCase())) {
			tree = hostData.get(hostname);
			node = tree.addEmptyNode(basePath);
		} else {
			tree = new DirectoryTree(hostname);
			node = tree.addEmptyNode(basePath);
			hostData.put(hostname, tree);
		}

		return node;
	}

	private static synchronized String getDirectoryFeatures(DirectoryTreeNode node) {
		return getDirectoryFeatures(node, false);
	}

	private static synchronized String getDirectoryFeatures(DirectoryTreeNode node, boolean isRel) {
		if (node == null)
			return null;

		DirectoryTree tree = node.getDirectoryTree();

		double relSegsChildNodes = DirectoryTree.getAvgRelSegsRatioChildNodes(node);
		double relPagesChildNodes = DirectoryTree.getAvgRelPagesRatioChildNodes(node);
		double relSegsSiblingNodes = DirectoryTree.getAvgRelSegsRatioSiblingNodes(node);
		double relPagesSiblingNodes = DirectoryTree.getAvgRelPagesRatioSiblingNodes(node);

		// if(tree.getOverallNonPages() + tree.getOverallRelPages() < 10)
		// return String.format("%s,?,?,?,?,?,?,?,?,?,?,non",
		// StringUtils.cleanUrlDataForPrediction(tree.getHostname() +
		// node.getPathName()));

		double slopeRel = -1;
		double slopeNon = -1;
		double slopeRatio = -1;
		double avgRel = -1;
		double avgNon = -1;
		double avgRatio = -1;
		
		
		if (tree.getCumulativeRelPages().size() >= 5) {
			ArrayList<Integer> tmpRel = tree.getCumulativeRelPages();
			ArrayList<Integer> tmpNon = tree.getCumulativeNonPages();

			int start = tmpRel.size();
			slopeRel = tmpRel.get(start - 1) - tmpRel.get(start - 5);
			slopeNon = tmpNon.get(start - 1) - tmpNon.get(start - 5);
			slopeRatio = HttpSegmentCrawler.calcRelevanceDegree(tmpRel.get(start - 1), tmpNon.get(start - 1)) - HttpSegmentCrawler.calcRelevanceDegree(tmpRel.get(start - 5), tmpNon.get(start - 5));
			slopeRel /= 5;
			slopeNon /= 5;
			slopeRatio /= 5;
			avgRel = 0;
			avgNon = 0;
			avgRatio = 0;
			for (int i = start - 1; i >= start - 5; i--) {
				avgRel += tmpRel.get(i);
				avgNon += tmpNon.get(i);
				
				avgRatio += HttpSegmentCrawler.calcRelevanceDegree(tmpRel.get(i), tmpNon.get(i));
			}
			
			avgRel/=5;
			avgNon/=5;
			avgRatio/=5;

		}
		
		

		String features = String.format("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s", StringUtils.cleanUrlDataForPrediction(tree.getHostname() + node.getPathName()),
				tree != null && tree.getOverallRelPagesRatio() != -1 ? String.valueOf(tree.getOverallRelPagesRatio()) : "?",
				tree != null && tree.getOverallRelSegsRatio() != -1 ? String.valueOf(tree.getOverallRelSegsRatio()) : "?",
				node != null && node.getRelPagesRatio() != -1 ? String.valueOf(node.getRelPagesRatio()) : "?",
				node != null && node.getRelSegsRatio() != -1 ? String.valueOf(node.getRelSegsRatio()) : "?",
				node.getParentNode() != null && node.getParentNode().getRelPagesRatio() != -1 ? String.valueOf(node.getParentNode().getRelPagesRatio()) : "?",
				node.getParentNode() != null && node.getParentNode().getRelSegsRatio() != -1 ? String.valueOf(node.getParentNode().getRelSegsRatio()) : "?",
				relSegsChildNodes != -1 ? relSegsChildNodes : "?", relPagesChildNodes != -1 ? relPagesChildNodes : "?", relSegsSiblingNodes != -1 ? relSegsSiblingNodes : "?",
				relPagesSiblingNodes != -1 ? relPagesSiblingNodes : "?", isRel ? "thai" : "non");

		features = String.format("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s", StringUtils.cleanUrlDataForPrediction(tree.getHostname() + node.getPathName()),
				tree != null && tree.getOverallRelPagesRatio() != -1 ? String.valueOf(tree.getOverallRelPagesRatio()) : "?",
				tree != null && tree.getOverallRelSegsRatio() != -1 ? String.valueOf(tree.getOverallRelSegsRatio()) : "?",
				node != null && node.getRelPagesRatio() != -1 ? String.valueOf(node.getRelPagesRatio()) : "?",
				node != null && node.getRelSegsRatio() != -1 ? String.valueOf(node.getRelSegsRatio()) : "?",
				node.getParentNode() != null && node.getParentNode().getRelPagesRatio() != -1 ? String.valueOf(node.getParentNode().getRelPagesRatio()) : "?",
				node.getParentNode() != null && node.getParentNode().getRelSegsRatio() != -1 ? String.valueOf(node.getParentNode().getRelSegsRatio()) : "?",
				relSegsChildNodes != -1 ? relSegsChildNodes : "?", relPagesChildNodes != -1 ? relPagesChildNodes : "?", relSegsSiblingNodes != -1 ? relSegsSiblingNodes : "?",
				relPagesSiblingNodes != -1 ? relPagesSiblingNodes : "?", slopeRel != -1 ? String.valueOf(slopeRel) : "?", slopeNon != -1 ? String.valueOf(slopeNon) : "?",
				slopeRatio != -1 ? String.valueOf(slopeRatio) : "?", avgRel != -1 ? String.valueOf(avgRel) : "?", avgNon != -1 ? String.valueOf(avgNon) : "?",
				avgRatio != -1 ? String.valueOf(avgRatio) : "?", isRel ? "thai" : "non");

		return features;

	}

	private static ArrayList<String> createHistoryDataSet() {
		logger.info("rel/non\t" + rel.size() + "/" + non.size());
		return FeaturesCollectors.underSampling(rel, non);
	}

	public static synchronized void backupAllHistoryData(String path) {
		ArrayList<String> output = new ArrayList<>();
		output.add(getHistoryFeaturesHeader());
		output.addAll(all);
		FileUtils.writeTextFile(path, output, false);

		FileUtils.writeTextFile(path + ".results.txt", results, false);
		FileUtils.writeTextFile(path + ".allDup.txt", allDup, false);
		// FileUtils.writeTextFile(path + ".previousHistory.txt",
		// previousHistory, false);

	}

	public static void trainHistoryPredictorForSegmentCrawler() {

		String path = CrawlerConfig.getConfig().getPredictorTrainingPath();
		if (path != null || path.trim().length() != 0) {
			path = "h" + path;
			trainHistoryPredictor(path);
		}
	}

	public static void trainHistoryPredictor(String path) {
		if (!useHistoryPredictor)
			return;

		if (path == null) {
			String[] tmp = FileUtils.readResourceFile("/resources/classifiers/history_initial.arff");
			FileUtils.writeTextFile("logs/htmp.arff", tmp, false);
			path = "logs/htmp.arff";
		}

		if (!FileUtils.exists(path)) {
			return;
		}

		// history
		logger.info("=========History==========");

		String algo = "weka.classifiers.bayes.NaiveBayesUpdateable";
		String option = null;
		// option = "-K";
		// algo = "weka.classifiers.lazy.IBk";
		// option = "-K " + 3 + " -W 0";

		int[] rm = new int[] { 1 };
		historyPredictor = new WekaClassifier(algo, option, 1.0);
		// String[] tmp =
		// FileUtils.readResourceFile("/resources/classifiers/history_initial.arff");
		// FileUtils.writeTextFile("htourism.arff", tmp, false);

		// if (option == null || !option.contains("-K"))
		historyPredictor.setDiscretize(5);

		historyPredictor.train(path, rm, true, null, true, false);

		// FileUtils.deleteFile("htmp.arff");
	}

	private static void parseDirectory(String path) {

		ArrayList<String> output = new ArrayList<>();
		int i = 0;
		for (String s : FileUtils.readFile(path)) {
			i++;
			if (i % 1000 == 0)
				System.out.println(i);
			if (!s.contains("relPagesRatio")) {
				continue;
			}

			try {
				s = s.substring(s.indexOf("INFO")).substring(s.indexOf("[") + 1);
				String result = s.substring(0, s.indexOf("]"));
				String segname = result.split("\t")[0];
				result = result.split("\t")[2];
				boolean isRel = result.contains("true");

				String data = s.substring(s.indexOf("]") + 1).trim();
				String[] d = data.split("\t");

				// current 1 3
				// host 5 7
				// parent 9 11
				// child 13 15
				// sibling 17 19

				String features = segname.trim() + "," + d[5].trim() + "," + d[7].trim();
				features += "," + d[1].trim() + "," + d[3].trim();
				features += "," + d[9].trim() + "," + d[11].trim();
				features += "," + d[13].trim() + "," + d[15].trim();
				features += "," + d[17].trim() + "," + d[19].trim();
				features += "," + (isRel ? "thai" : "non");
				// System.out.println(features);
				output.add(features);
			} catch (Exception e) {
				e.printStackTrace();
				System.err.println(s);
			}

			FileUtils.writeTextFile("hestate-raw.arff", output, false);
		}

	}

	private static void cleanDirectoryFeature(String path) {
		ArrayList<String> output = new ArrayList<>();
		for (String[] s : FileUtils.readArffData(path)) {
			String features = "";
			s[0] = StringUtils.cleanUrlDataForPrediction(s[0]);
			// s[0] = "'" + s[0].replace("{", "").replace("}", "").replace(",",
			// "").replace("%", "").replace("'", "").replace("`",
			// "").replace("\"", "") + "'";
			for (int i = 0; i < s.length; i++) {
				if (i != s.length - 1)
					features += s[i] + ",";
				else
					features += s[i];
			}
			output.add(features);
		}

		FileUtils.writeTextFile(path, header + "\n", false);
		FileUtils.writeTextFile(path, output, true);
	}

	private static void kFold(String filename, String destPath) throws IOException {
		int k = 3;
		boolean split = true;

		FileUtils.mkdir(destPath);

		ArrayList<String> all = FileUtils.readArffDataWithoutSplit(filename);
		String header = "@relation 'history'\n@attribute segpath string\n@attribute HostrelPagesRatio numeric\n@attribute HostrelSegsRatio numeric\n@attribute relPagesRatio numeric\n@attribute relSegsRatio numeric\n@attribute ParentRelPagesRatio numeric\n@attribute ParentRelSegsRatio numeric\n@attribute ChildRelPagesRatio numeric\n@attribute ChildRelSegsRatio numeric\n@attribute SiblingRelPagesRatio numeric\n@attribute SiblingRelSegsRatio numeric\n@attribute class {thai,non}\n@data\n";

		if (split) {
			ArrayList<String> rel = new ArrayList<>();
			ArrayList<String> non = new ArrayList<>();

			for (String s : all) {

				if (s.contains(",?,?,?,?,?,?,?,?,?,?,"))
					continue;

				String[] tmp = s.split(",");

				if (tmp[tmp.length - 1].trim().equals("non")) {
					non.add(s);
				} else {
					rel.add(s);
				}
			}

			System.out.println(rel.size());
			System.out.println(non.size());
			Collections.shuffle(rel);
			Collections.shuffle(non);

			int nonPerFile = (int) Math.ceil(non.size() * 1.0 / k);
			int thaiPerFile = (int) Math.ceil(rel.size() * 1.0 / k);

			for (int i = 0; i < k; i++) {
				BufferedWriter br = FileUtils.getBufferedFileWriter(destPath + "/history-" + i + "-rel.arff");
				BufferedWriter bn = FileUtils.getBufferedFileWriter(destPath + "/history-" + i + "-non.arff");
				br.write(header);
				bn.write(header);

				for (int j = 0; j < nonPerFile && non.size() > 0; j++) {
					bn.write(non.remove(0) + "\n");
				}

				for (int j = 0; j < thaiPerFile && rel.size() > 0; j++) {
					br.write(rel.remove(0) + "\n");
				}

				br.close();
				bn.close();
			}
		}

		double g = 0;

		for (int i = 0; i < k; i++) {
			ArrayList<String> rtrain = new ArrayList<>();
			ArrayList<String> ntrain = new ArrayList<>();

			ArrayList<String> rtest = new ArrayList<>();
			ArrayList<String> ntest = new ArrayList<>();

			for (int j = 0; j < k; j++) {

				ArrayList<String> r = FileUtils.readArffDataWithoutSplit(destPath + "/history-" + i + "-rel.arff");
				ArrayList<String> n = FileUtils.readArffDataWithoutSplit(destPath + "/history-" + i + "-non.arff");

				if (i != j) {
					// for(String s: r)
					// rtrain.add(s.replace("?", "-1"));
					//
					// for(String s: n)
					// ntrain.add(s.replace("?", "-1"));

					rtrain.addAll(r);
					ntrain.addAll(n);

				} else {

					// for(String s: r)
					// rtest.add(s.replace("?", "-1"));

					// for(String s: n)
					// ntest.add(s.replace("?", "-1"));

					rtest.addAll(r);
					ntest.addAll(n);

					FileUtils.writeTextFile(destPath + "/test.arff", header, false);
					FileUtils.writeTextFile(destPath + "/test.arff", rtest, true);
					FileUtils.writeTextFile(destPath + "/test.arff", ntest, true);
					ArrayList<String> testU = FeaturesCollectors.underSampling(r, n);

					// System.out.println(rtest.size() + "\t" + ntest.size() +
					// "\t" + (testU == null));
					FileUtils.writeTextFile(destPath + "/test_u.arff", header, false);
					FileUtils.writeTextFile(destPath + "/test_u.arff", testU, true);
				}

			}

			System.out.println(rtrain.size() + "\t" + ntrain.size());
			ArrayList<String> output = FeaturesCollectors.underSampling(rtrain, ntrain);
			FileUtils.writeTextFile(destPath + "/train.arff", header, false);
			FileUtils.writeTextFile(destPath + "/train.arff", output, true);

			FileUtils.copyFile(destPath + "/train.arff", destPath + "/" + filename.replace("-raw", ""));
			FileUtils.writeTextFile(destPath + "/" + filename.replace("-raw", ""), FileUtils.readArffDataWithoutSplit(destPath + "/test_u.arff"), true);

			// train

			String algo = "weka.classifiers.bayes.NaiveBayesUpdateable";
			int[] rm = { 1 };
			WekaClassifier link = new WekaClassifier(algo, null, 1.0);
			try {
				link.train(destPath + "/train.arff", rm, true, null, true, false);

				g += link.getGMeanFromTestSet(destPath + "/test.arff");
			} catch (Exception e) {
				System.exit(0);
			}

		}

		logger.info(g / k);

	}

	public static void parseLogTrainingHistory(String path) {

		double d = 0;
		int count = 0;
		for (String s : FileUtils.readFile(path)) {
			s = s.toLowerCase();
			if (s.contains("tp") && s.contains("tn") && s.contains("fp") && s.contains("fn")) {
				s = s.substring(s.indexOf("tp"));
				String[] tmp = s.replace("tp", "").replace("tn", "").replace("fp", "").replace("fn", "").split("\t");
				ConfusionMatrixObj o = new ConfusionMatrixObj(Integer.parseInt(tmp[0].trim()), Integer.parseInt(tmp[1].trim()), Integer.parseInt(tmp[2].trim()), Integer.parseInt(tmp[3].trim()));
				System.out.println(o.getGmean());
				count++;
				d += o.getGmean();
			}

		}

		System.out.println("Average:\t" + d / count);
	}

	public static void main(String[] args) {

		// parseLogSeries("history.log", true);
		// parseLogSeries("history.log", false);
		// parseLogSeries("history.log", false);
		// parseLogSeries("history.log", false);

		
		DirectoryHistory.record("http://www.a.com", 10, 5);
		DirectoryHistory.record("http://www.a.com", 10, 5);
		DirectoryHistory.record("http://www.a.com", 10, 5);
		DirectoryHistory.record("http://www.a.com", 10, 5);
		DirectoryHistory.record("http://www.a.com", 10, 5);
		
		DirectoryTreeNode node = DirectoryHistory.getDirectoryNode("http://www.a.com");
		System.out.println(getDirectoryFeatures(node));
		 System.exit(0);

		// parsePreviousHistory("logs-test/history.arff.previousHistory.txt",
		// "tmp.txt");
		// anaylyzeDup("logs-dup-test-tourism/history.arff.allDup.txt");
		// parseLogTrainingHistory("history/estate/logs/ic.log");

		String training = "hgaming-page-raw.arff";
		cleanDirectoryFeature(training);
		// cleanDirectoryFeature("hgaming-bf-raw.arff");
		// cleanDirectoryFeature("htourism-bf-raw.arff");
		// cleanDirectoryFeature("hdiving-bf-raw.arff");
		// cleanDirectoryFeature("hestate-bf-raw.arff");

		// System.exit(0);
		try {

			// logger.info("-----best-first-----");
			//
			// logger.info("baseball");
			// kFold("hbaseball-bf-raw.arff", "hbf-baseball");
			//
			// logger.info("gaming");
			// kFold("hgaming-bf-raw.arff", "hbf-gaming");
			//
			// logger.info("tourism");
			// kFold("htourism-bf-raw.arff", "hbf-tourism");
			//
			// logger.info("estate");
			// kFold("hestate-bf-raw.arff", "hbf-estate");

			for (int i = 0; i < 5; i++) {
				kFold(training, training.replace("-raw", "").replace("-bf", "") + i);
			}

			// kFold("hgaming-bf-raw.arff", "hgaming-page1");
			// kFold("hgaming-bf-raw.arff", "hgaming-page2");
			// kFold("hgaming-bf-raw.arff", "hgaming-page");
			// kFold("hgaming-bf-raw.arff", "hgaming-page");

		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

	public static void parseLogSeries(String input, boolean onlyRel) {
		ArrayList<Double> degrees = new ArrayList<>();
		ArrayList<Double> avgs = new ArrayList<>();
		ArrayList<Integer> lengths = new ArrayList<>();
		ArrayList<Double> relevancy = new ArrayList<>();

		int rel = 0, non = 0;
		int trel = 0, tnon = 0;

		int l = 0, t = 0;

		for (String s : FileUtils.readFile(input)) {

			if (s.contains("DOWNLOADED")) {

				s = s.substring(s.indexOf("DOWNLOADED") + "DOWNLOADED".length());
				s = s.trim();
				String[] ts = s.split("\t");

				double score = Double.parseDouble(ts[0]);
				if (score > 0.5)
					trel++;
				else
					tnon++;
			}

			if (!s.contains("HttpSegmentCrawler:305-"))
				continue;
			s = s.substring(s.indexOf("HttpSegmentCrawler:305-") + "HttpSegmentCrawler:305-".length()).trim();

			String[] data = s.split("\t");

			double degree = Double.parseDouble(data[1]);
			boolean isRel = Boolean.parseBoolean(data[2]);

			if (onlyRel != isRel)
				continue;

			ArrayList<Integer> relPages = new ArrayList<>();
			ArrayList<Integer> nonPages = new ArrayList<>();

			ArrayList<Integer> relSegs = new ArrayList<>();
			ArrayList<Integer> nonSegs = new ArrayList<>();

			if (data[3].equals("[]"))
				continue;

			for (String tmp : data[3].replace("[", "").replace("]", "").split(",")) {
				relPages.add(Integer.parseInt(tmp.trim()));
			}

			for (String tmp : data[4].replace("[", "").replace("]", "").split(",")) {
				nonPages.add(Integer.parseInt(tmp.trim()));
			}

			for (String tmp : data[5].replace("[", "").replace("]", "").split(",")) {
				relSegs.add(Integer.parseInt(tmp.trim()));
			}

			for (String tmp : data[6].replace("[", "").replace("]", "").split(",")) {
				nonSegs.add(Integer.parseInt(tmp.trim()));
			}

			lengths.add(relPages.size());

			double current_relevancy = HttpSegmentCrawler.calcRelevanceDegree(relPages.get(relPages.size() - 1), nonPages.get(relPages.size() - 1));
			double diff = current_relevancy - HttpSegmentCrawler.calcRelevanceDegree(relPages.get(0), nonPages.get(0));
			diff /= relPages.size();

			avgs.add(diff);
			degrees.add(degree);

			relevancy.add(current_relevancy);

			int idegree = (int) (current_relevancy * 10);
			// System.out.println(current_relevancy + "\t" + idegree);
			if (idegree > 0 && idegree <= 2) {
				lengths.add(relPages.size());

				if (relPages.size() > 1) {
					int diffRel = relPages.get(relPages.size() - 1) - relPages.get(relPages.size() - 2);

					int diffNon = nonPages.get(nonPages.size() - 1) - nonPages.get(nonPages.size() - 2);

					rel += diffRel;
					non += diffNon;

				}
			}
		}

		System.out.println(rel + "\t" + non);
		System.out.println(trel + "\t" + tnon);
		System.out.println("---");
		// FileUtils.writeTextFile("length-non.txt",
		// output.stream().map(a->String.valueOf(a)).collect(Collectors.toList()),
		// false);

		HashMap<Integer, Integer> count = new HashMap<>();
		for (double a : lengths) {
			int key = (int) (a * 10);

			if (!count.containsKey(key))
				count.put(key, 0);

			count.put(key, count.get(key) + 1);
		}

		System.out.println("-------------");

		for (int i = 0; i <= 10; i++) {
			if (count.containsKey(i))
				System.out.println(i + "\t" + count.get(i));
			else
				System.out.println(i + "\t" + 0);

		}

		// HashMap<Integer, Integer> countAvgs = new HashMap<>();
		// for (double a : avgs) {
		//
		// int key = (int) (a * 10);
		//
		// if (!countAvgs.containsKey(key))
		// countAvgs.put(key, 0);
		//
		// countAvgs.put(key, countAvgs.get(key) + 1);
		// }

		// System.out.println(count.size());
		// for(int a:
		// count.keySet().stream().sorted().collect(Collectors.toList())){
		// System.out.println(a + "\t" + count.get(a));
		// }

		// for(int i=0; i<=260; i++){
		// if(count.containsKey(i)){
		// System.out.println(i + "\t" + count.get(i));
		// }else{
		// System.out.println(i + "\t" + 0);
		// }
		// }
	}

	public static void parsePreviousHistory(String inputPath, String outputPath) {
		ArrayList<String> rel = new ArrayList<>();
		ArrayList<String> non = new ArrayList<>();

		for (String s : FileUtils.readFile(inputPath)) {

			String[] t = s.split("\t");
			String seg = t[0];

			String data = t[1].split("]")[0].replace("[", "");
			// String hostPages = d[0];
			// String hostSegs = d[1];
			// String pathPages = d[2];
			// String pathSegs = d[3];
			if (data.trim().length() == 0)
				continue;

			boolean isRel = t[2].contains("true");
			if (isRel) {
				rel.add(data);
			} else {
				non.add(data);
			}

			// System.out.println(seg + "\t" + hostPages + "\t" + hostSegs +
			// "\t" + pathPages + "\t" + pathSegs + "\t" + isRel);;
		}

		FileUtils.writeTextFile(outputPath + ".rel.txt", rel, false);
		FileUtils.writeTextFile(outputPath + ".non.txt", non, false);

	}

	public static void anaylyzeDup(String path) {

		boolean first = true;

		HashMap<Integer, Integer> numFeatures = new HashMap<>();
		HashMap<Integer, Integer> numFeaturesNon = new HashMap<>();
		HashMap<Integer, Integer> numFeaturesRel = new HashMap<>();

		int dup = 0, dupRel = 0, dupNon = 0, uRel = 0, uNon = 0, all = 0;
		for (String features : FileUtils.readFile(path)) {

			String hostname = features.substring(0, features.indexOf(","));
			String host = HttpUtils.getHost(hostname);

			if (host != null) {
				if (!dupDb.containsKey(host)) {
					dupDb.put(host, new HashSet<>());
				}

				all++;
				// ตัด segname ออก
				String tmp = features.substring(features.indexOf(",") + 1);

				boolean isRel = false;

				if (tmp.contains(",thai")) {
					isRel = true;
				}

				// filter first

				if (first) {
					// TODO: Dup
					if (dupDb.get(host).contains(tmp)) {
						dup++;

						if (isRel)
							dupRel++;
						else
							dupNon++;
						continue;
					} else {
						if (isRel)
							uRel++;
						else
							uNon++;
					}

				}

				int count = StringUtils.countWordInStr(tmp, "?");

				// System.err.println(tmp + "\t" + count);
				if (!numFeatures.containsKey(count)) {
					numFeatures.put(count, 0);
					numFeaturesRel.put(count, 0);
					numFeaturesNon.put(count, 0);
				}

				if (isRel)
					numFeaturesRel.put(count, numFeaturesRel.get(count) + 1);
				else
					numFeaturesNon.put(count, numFeaturesNon.get(count) + 1);

				numFeatures.put(count, numFeatures.get(count) + 1);

				// filter later

				if (!first) {
					// TODO: Dup
					if (dupDb.get(host).contains(tmp)) {
						dup++;

						if (isRel)
							dupRel++;
						else
							dupNon++;
						continue;
					} else {
						if (isRel)
							uRel++;
						else
							uNon++;
					}

				}

				dupDb.get(host).add(tmp);

			}
		}

		System.out.println("#All data:\t" + all);
		System.out.println("#Dup data:\t" + dup + "\tRel:\t" + dupRel + "\tNon:\t" + dupNon);
		System.out.println("#Usable data:\t" + (all - dup) + "\tRel:\t" + uRel + "\tNon:\t" + uNon);

		System.out.println("--------------");

		for (int i : numFeatures.keySet()) {
			System.out.printf("Features count: %d\t%d\tRel:\t%d\tNon:\t%d\n", i, numFeatures.get(i), numFeaturesRel.get(i), numFeaturesNon.get(i));
		}
	}
}
